{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Problem 2.4\n",
        "\n",
        "Implement the perceptron algorithm and use it to perform classification on the \n",
        "IRIS PLANT DATABASE data as follows: Examine whether the data of each class are \n",
        "linearly separable from the data of the combined remaining classes (e.g. if the \n",
        "Iris Setosa data are linearly separable from the combined Iris Versicolor and \n",
        "Iris Virginica data). \n",
        "\"\"\"\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "GoKUotm31d2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Functions\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Loads the data set and converts the feature vector to floats.\n",
        "Parameter:\n",
        "-filename: The path to the dataset file\n",
        "Returns:\n",
        "-The dataset as an ndarray \n",
        "-The labels of the classes in a list \n",
        "-The number of features\n",
        "\"\"\"\n",
        "def load_dataset(filename):\n",
        "    dataset = []\n",
        "    with open(filename) as file:\n",
        "        for line in file:\n",
        "            line = line.rstrip().split(\",\") #For every line of the data file delete the /n character and split the string into a list\n",
        "            \n",
        "            #Convert the stings into floats (for the sepal and petal lenghts)\n",
        "            for i in range(0,len(line)-1):\n",
        "                line[i] = float(line[i])\n",
        "            if len(line) > 1:\n",
        "                dataset.append(line)\n",
        "    \n",
        "    dataset = np.array(dataset) #Convert list to ndarray\n",
        "    class_labels = np.unique(dataset[:, -1]) #Get the labels of the classes\n",
        "\n",
        "    return dataset, class_labels, len(dataset[0,:-1]) #Return the dataset, the labels of the classes and the number of features\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Change the labels: 1 for the class under study, -1 for the other two classes.\n",
        "Parameter:\n",
        "-dataset: The whole datatset\n",
        "-class_labels: The labels of the classes in a list \n",
        "-class_label: The label of the class under study\n",
        "Returns:\n",
        "- The dataset with its labels changed.\n",
        "\"\"\"\n",
        "def change_labels(dataset, class_labels, class_label):\n",
        "    dataset_cp = dataset.copy()\n",
        "    label_index = np.where(class_labels == class_label)\n",
        "    temp = np.where(dataset_cp[:, -1] == class_labels[label_index], 1, -1)\n",
        "    dataset_cp[:, -1] = temp\n",
        "    return dataset_cp\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Implementation of the Perceptron algorithm for they weight estimation.\n",
        "Paramateres:\n",
        "-dataset: The dataset with the changed labels\n",
        "-num_features: The number of features\n",
        "-epochs: The max limit of epochs\n",
        "Returns:\n",
        "-The weight vector without the bias term\n",
        "-The bias\n",
        "\"\"\"\n",
        "def train_weights(dataset, num_features, epochs = 1000):\n",
        "    weights = np.zeros(num_features + 1) #Create an array of length number of features + 1(bias) to store the weights\n",
        "\n",
        "    x_vector = np.array(dataset[:, :-1]) #Create an array of the features\n",
        "    x_vector = np.hstack((x_vector, np.ones((x_vector.shape[0],1)) * -1)) #Add a column of -1's\n",
        "\n",
        "    y_vector = np.array(dataset[:, -1]).reshape(x_vector.shape[0],1) #Create an array of the classes of each feature vector \n",
        "\n",
        "    target_x = x_vector * y_vector #Create the t*x array\n",
        "\n",
        "    for epoch in range(0, epochs):     #For as long as the epoch number is lower than the limit\n",
        "\n",
        "        errors = 0\n",
        "\n",
        "        #Create the t*w*x vector\n",
        "        for row in target_x:\n",
        "            twx = np.dot(weights, row)\n",
        "\n",
        "            #If it is smaller or equal to zero add an error and change the weights\n",
        "            if twx <= 0.0:\n",
        "                errors += 1\n",
        "                weights = weights + row\n",
        "\n",
        "        #If the number of errors after an epoch is equal to zero:\n",
        "            #Stop the iteration\n",
        "            #This class is linear separable\n",
        "        if errors == 0:\n",
        "            print(\"Converged when epoch = \" + str(epoch + 1))\n",
        "            print(\"This class is linear separable\")\n",
        "            break\n",
        "\n",
        "        #If the number of errors isn't equal to zero and the number of epochs reached the limit:\n",
        "            #Stop the iteration\n",
        "            #This class is not linear separable\n",
        "        if errors != 0 and epoch >= epochs -1:\n",
        "            print(\"Failed to converge - This class is NOT linear separable\")\n",
        "            \n",
        "    print(\"Epoch \" +  str(epoch + 1) + \", errors = \" + str(errors))\n",
        "    return weights[:-1], weights[-1]"
      ],
      "metadata": {
        "id": "mLT8NUAy1ik7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, labels, num_features= load_dataset(\"iris.data\")"
      ],
      "metadata": {
        "id": "TRzLMjrb1lRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for label in labels: #For each class\n",
        "    print(\"Checking for class:\", label)\n",
        "    temp_dataset = change_labels(dataset, labels, label)\n",
        "    temp_dataset = temp_dataset.astype(np.float)\n",
        "    weights, w0 = train_weights(temp_dataset, num_features)\n",
        "    print('weights: ', weights)\n",
        "    print('bias(w0): ', w0)\n",
        "    print()"
      ],
      "metadata": {
        "id": "1WLdajOouF_V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04949c55-60c5-4e07-a4b9-357957e1a4ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for class: Iris-setosa\n",
            "Converged when epoch = 4\n",
            "This class is linear separable\n",
            "Epoch 4, errors = 0\n",
            "weights:  [ 1.3  4.1 -5.2 -2.2]\n",
            "bias(w0):  -1.0\n",
            "\n",
            "Checking for class: Iris-versicolor\n",
            "Failed to converge - This class is NOT linear separable\n",
            "Epoch 1000, errors = 8\n",
            "weights:  [  63.1  -57.6   -8.  -145.6]\n",
            "bias(w0):  98.0\n",
            "\n",
            "Checking for class: Iris-virginica\n",
            "Failed to converge - This class is NOT linear separable\n",
            "Epoch 1000, errors = 3\n",
            "weights:  [ -99.3 -125.9  155.1  246.4]\n",
            "bias(w0):  180.0\n",
            "\n"
          ]
        }
      ]
    }
  ]
}